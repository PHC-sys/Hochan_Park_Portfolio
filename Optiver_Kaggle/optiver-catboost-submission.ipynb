{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:39:54.619990Z","iopub.execute_input":"2023-11-11T01:39:54.620451Z","iopub.status.idle":"2023-11-11T01:39:55.009102Z","shell.execute_reply.started":"2023-11-11T01:39:54.620401Z","shell.execute_reply":"2023-11-11T01:39:55.008080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n# from hmmlearn.hmm import GaussianHMM\nimport optuna\n\nfrom catboost import CatBoostRegressor\n\nfrom datetime import datetime\nfrom datetime import timedelta\nimport copy\n\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MinMaxScaler\n\ntrain = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n# test set 나누기\n# test = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:43:38.178544Z","iopub.execute_input":"2023-11-11T01:43:38.178931Z","iopub.status.idle":"2023-11-11T01:43:50.543668Z","shell.execute_reply.started":"2023-11-11T01:43:38.178901Z","shell.execute_reply":"2023-11-11T01:43:50.542821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_dict = {}","metadata":{"execution":{"iopub.status.busy":"2023-11-12T03:17:06.770871Z","iopub.execute_input":"2023-11-12T03:17:06.771135Z","iopub.status.idle":"2023-11-12T03:17:06.781047Z","shell.execute_reply.started":"2023-11-12T03:17:06.771109Z","shell.execute_reply":"2023-11-12T03:17:06.780170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # DataFrame에 numeric한 데이터만 있어야 함\n# def normalize_data(df):\n#   try:\n#     df.far_price = df.far_price.fillna(df.far_price.dropna().iloc[0])\n#     df.near_price = df.near_price.fillna(df.near_price.dropna().iloc[0])\n#     normalized_data = minmax_scale(df)\n#     normalized_df = pd.DataFrame(normalized_data, columns = df.columns, index = df.index)\n#   except:\n#     normalized_data = minmax_scale(df)\n#     normalized_df = pd.DataFrame(normalized_data, columns = df.columns, index = df.index)\n  \n#   return normalized_df\n\n\n# # Date별로 normalize, date_id가 포함되어 있어야 함\n# def normalize_date_data(df, stock_id):\n#   result_df = pd.DataFrame()\n#   date_list = df['date_id'].unique()\n#   for date in date_list:\n#     # 만약 마지막 date면, scaler 만들어주기\n#     if date == date_list[-1]:\n#         scaler = MinMaxScaler()\n#         date_split_data = df[df['date_id']==date]\n#         patch_list = ['imbalance_size', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n#         try:\n#             df.far_price = df.far_price.fillna(df.far_price.dropna().iloc[0])\n#             df.near_price = df.near_price.fillna(df.near_price.dropna().iloc[0])\n#             scaler.fit(df[patch_list])\n#             normalized_data = scaler.transform(df[patch_list])\n#             normalized_df = pd.DataFrame(normalized_data, columns = patch_list, index = df.index)\n#         except:\n#             scaler.fit(df[patch_list])\n#             normalized_data = scaler.transform(df[patch_list])\n#             normalized_df = pd.DataFrame(normalized_data, columns = patch_list, index = df.index)\n# #         globals()[\"scaler_{}\".format(stock_id)] = scaler\n#         normalized_df['date_id'] = df['date_id']\n#         normalized_df['target'] = df['target']\n#         normalized_df['imbalance_buy_sell_flag'] = df['imbalance_buy_sell_flag']\n#         normalized_df['time_id'] = df['time_id']\n#         scaler_dict[stock_id] = scaler\n#         result_df = pd.concat([result_df, normalized_df])\n#     else:\n#         date_split_data = df[df['date_id']==date]\n#         normalized_df = normalize_data(date_split_data.drop(['date_id', 'imbalance_buy_sell_flag', 'time_id'], axis=1))\n#         normalized_df['date_id'] = df['date_id']\n#         normalized_df['target'] = df['target']\n#         normalized_df['imbalance_buy_sell_flag'] = df['imbalance_buy_sell_flag']\n#         normalized_df['time_id'] = df['time_id']\n#         result_df = pd.concat([result_df, normalized_df])  \n#   return result_df\n\n\n# # stock_id 별로 \n# def normalize_stock_data(df):\n#     result_df = pd.DataFrame()\n#     stock_list = df['stock_id'].unique()\n#     for ticker in stock_list:\n#       stock_split_data = df[df['stock_id']==ticker]\n#       normalized_data = normalize_date_data(stock_split_data[['date_id','imbalance_buy_sell_flag', 'time_id', 'imbalance_size', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap', 'target']], ticker)\n#       normalized_data['stock_id'] = ticker\n#       result_df = pd.concat([result_df, normalized_data])\n\n#     return result_df\n\n\n# def normalize_test_data(df):\n#     stock_list = df['stock_id'].unique()\n#     patch_list = ['imbalance_size', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n#     for ticker in stock_list:\n#       stock_split_data = df[df['stock_id']==ticker]\n#       scaler = scaler_dict[ticker]\n#       normalized_data = scaler.transform(stock_split_data[patch_list])\n#       normalized_df = pd.DataFrame(normalized_data, columns = patch_list, index = stock_split_data.index)\n#       normalized_df['date_id'] = stock_split_data['date_id']\n#       normalized_df['imbalance_buy_sell_flag'] = stock_split_data['imbalance_buy_sell_flag']\n#       normalized_df['stock_id'] = ticker\n#     return normalized_df","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:50:01.615461Z","iopub.execute_input":"2023-11-11T01:50:01.616368Z","iopub.status.idle":"2023-11-11T01:50:01.629589Z","shell.execute_reply.started":"2023-11-11T01:50:01.616333Z","shell.execute_reply":"2023-11-11T01:50:01.628611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train.sort_values(['stock_id', 'date_id', 'time_id'])\n# # test = test.sort_values(['stock_id', 'date_id', 'time_id'])\n# df = train.drop(['date_id', 'time_id',], axis=1)\n# # test_1 = test.drop(['date_id', 'time_id', 'row_id'], axis=1)\n\n# # test_df = copy.deepcopy(test_1)\n\n# CatBoostRegressor = CatBoostRegressor(iterations=5,\n#                           learning_rate=0.1,\n#                           depth=5)\n\n# df = df.dropna()\n# # X_train = df.drop(['target', 'stock_id'], axis=1)\n# X_train = df.drop(['target'], axis=1)\n# y_train = df['target']\n\n# # X_test = test_df.drop(['stock_id'], axis=1)\n\n# CatBoostRegressor.fit(X_train, y_train)\n\n# # y_pred = CatBoostRegressor.predict(X_test)\n\n# # print(y_pred)\n\n\n# counter = 0\n# for (test, revealed_targets, sample_prediction) in iter_test:\n#     test_df = test.sort_values(['stock_id', 'date_id', 'seconds_in_bucket'])\n#     test_1 = test_df.drop(['stock_id','date_id', 'row_id'], axis=1)\n#     sample_prediction['target'] = CatBoostRegressor.predict(test_1)\n#     env.predict(sample_prediction)\n    \n#     counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:46:53.773665Z","iopub.status.idle":"2023-11-11T01:46:53.774054Z","shell.execute_reply.started":"2023-11-11T01:46:53.773882Z","shell.execute_reply":"2023-11-11T01:46:53.773900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_train = normalize_stock_data(train)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T01:43:50.558380Z","iopub.execute_input":"2023-11-11T01:43:50.558636Z","iopub.status.idle":"2023-11-11T01:46:53.673115Z","shell.execute_reply.started":"2023-11-11T01:43:50.558614Z","shell.execute_reply":"2023-11-11T01:46:53.671718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = new_train.sort_values(['stock_id', 'date_id', 'time_id'])\n# # df = train.drop(['date_id', 'time_id',], axis=1)\n\n# CatBoostRegressor = CatBoostRegressor(iterations=100,\n#                           learning_rate=0.01,\n#                           depth=15)\n\n# df = train.dropna()\n# df = df[['stock_id', 'date_id', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap', 'target']]\n\n# X_train = df.drop(['target'], axis=1)\n# y_train = df['target']\n\n# CatBoostRegressor.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# counter = 0\n# for (test, revealed_targets, sample_prediction) in iter_test:\n#     test_df = test.sort_values(['stock_id', 'date_id', 'seconds_in_bucket'])\n#     test_df = test_df.drop(['seconds_in_bucket', 'row_id'], axis=1)\n#     sample_prediction['target'] = CatBoostRegressor.predict(test_df)\n#     env.predict(sample_prediction)\n    \n#     counter += 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values(['stock_id', 'date_id', 'time_id'])\ndf = train.drop(['date_id', 'time_id', 'row_id'], axis=1)\n\nCatBoostRegressor = CatBoostRegressor(iterations=100,\n                          learning_rate=0.01,\n                          depth=15)\n\ndf = df.dropna()\ndf = df[['stock_id', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap', 'target']]\n\nX_train = df.drop(['target'], axis=1)\ny_train = df['target']\n\nCatBoostRegressor.fit(X_train, y_train)\n\ncounter = 0\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    test_df = test.sort_values(['stock_id', 'date_id', 'seconds_in_bucket'])\n    test_1 = test_df.drop(['seconds_in_bucket', 'date_id', 'row_id'], axis=1)\n    sample_prediction['target'] = CatBoostRegressor.predict(test_1)\n    env.predict(sample_prediction)\n    \n    counter += 1","metadata":{},"execution_count":null,"outputs":[]}]}